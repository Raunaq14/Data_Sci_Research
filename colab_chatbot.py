# -*- coding: utf-8 -*-
"""Colab_ChatBot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_P4ck3lQtRxyOytRKJiVw97MC6wnTNuy
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

# Load merged 2024 data (change to 2023 if needed)
file_path = "/content/drive/MyDrive/Extracted_FDA_Data/final_merged_data/final_data_with_foi_2024.csv"
df = pd.read_csv(file_path, dtype=str, low_memory=False)

print("Data loaded successfully. Shape:", df.shape)
df.head()

# Check how many records exist for each device brand
device_counts = df['BRAND_NAME'].value_counts().head(10)
print(device_counts)

# Define the 4 devices you're focusing on
target_devices = [
    "DEXCOM G6 CONTINUOUS GLUCOSE MONITORING SYSTEM",
    "DEXCOM G7 CONTINUOUS GLUCOSE MONITORING SYSTEM",
    "OMNIPOD 5 POD",
    "LIBRE 2 SENSOR FREESTYLE"
]

# Filter
df_filtered = df[df['BRAND_NAME'].isin(target_devices)].copy()
print("‚úÖ Filtered dataset shape:", df_filtered.shape)

# Check count per device
print(df_filtered['BRAND_NAME'].value_counts())

# Save if needed
df_filtered.to_csv("/content/drive/MyDrive/Extracted_FDA_Data/filtered_devices_2024.csv", index=False)

import matplotlib.pyplot as plt
import seaborn as sns

# Check missing value percentage
missing_percent = df_filtered.isnull().mean().sort_values(ascending=False) * 100
missing_percent = missing_percent[missing_percent > 0]

# Plot top 25 columns with missing data
plt.figure(figsize=(12, 6))
sns.barplot(x=missing_percent[:25].values, y=missing_percent[:25].index)
plt.title("Top 25 Columns with Missing Values (%)")
plt.xlabel("Percentage")
plt.ylabel("Column")
plt.tight_layout()
plt.show()

# Drop columns with more than 90% missing values
threshold = 0.9
missing_ratio = df_filtered.isnull().mean()
cols_to_drop = missing_ratio[missing_ratio > threshold].index.tolist()

print(f"üîç Dropping {len(cols_to_drop)} columns with >90% missing values...")
df_filtered.drop(columns=cols_to_drop, inplace=True)
print("‚úÖ Remaining columns:", df_filtered.shape[1])

df_filtered.columns

import re

def clean_foitext(text):
    if pd.isna(text):
        return ""
    # Lowercase
    text = text.lower()

    # Remove boilerplate FDA phrases
    boilerplate_patterns = [
        r'the information received.*?intended to.*?\.',  # example phrase pattern
        r'this text has not been.*?accuracy\.',          # generic FDA disclaimers
    ]
    for pattern in boilerplate_patterns:
        text = re.sub(pattern, '', text, flags=re.DOTALL)

    # Remove punctuation, digits, and extra whitespace
    text = re.sub(r'[\d]+', '', text)  # remove numbers
    text = re.sub(r'[^\w\s]', '', text)  # remove punctuation
    text = re.sub(r'\s+', ' ', text)  # collapse whitespace

    return text.strip()

# Apply the cleaning function
df_filtered['cleaned_foitext'] = df_filtered['FOI_TEXT'].apply(clean_foitext)

# Check a sample
df_filtered[['FOI_TEXT', 'cleaned_foitext']].head()

df_filtered.to_csv("/content/drive/MyDrive/Extracted_FDA_Data/filtered_cleaned_devices_2024.csv", index=False)

import nltk
nltk.download('punkt')
nltk.download('punkt_tab')  # explicitly request this
nltk.download('stopwords')

from collections import Counter
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

stop_words = set(stopwords.words('english'))

def get_top_words(texts, top_n=20):
    all_words = []
    for text in texts:
        words = word_tokenize(text.lower())
        words = [word for word in words if word.isalpha() and word not in stop_words]
        all_words.extend(words)
    return Counter(all_words).most_common(top_n)

device_keywords = {}
for device in df_filtered['BRAND_NAME'].unique():
    texts = df_filtered[df_filtered['BRAND_NAME'] == device]['cleaned_foitext'].dropna()
    device_keywords[device] = get_top_words(texts)

# Show top 10 for each device
for device, keywords in device_keywords.items():
    print(f"\nüîç {device} - Top Keywords:")
    for word, count in keywords[:10]:
        print(f"{word}: {count}")

from wordcloud import WordCloud
import matplotlib.pyplot as plt

for device, keywords in device_keywords.items():
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(dict(keywords))
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(f"Common Complaint Terms: {device}")
    plt.tight_layout()
    plt.show()

"""## Step 1A: Define Keyword-to-Category Mapping"""

# Define keyword mapping to issue categories
keyword_to_category = {
    "sensor": "Malfunction",
    "pod": "Malfunction",
    "cannula": "Malfunction",
    "infusion": "Malfunction",
    "site": "Malfunction",
    "pdm": "Malfunction",
    "device": "Malfunction",

    "injury": "Medical Risk",
    "intervention": "Medical Risk",
    "cause": "Medical Risk",
    "medical": "Medical Risk",
    "occurred": "Medical Risk",
    "allegation": "Medical Risk",

    "complaint": "Complaint",
    "report": "Complaint",
    "returned": "Complaint",
    "risk": "Complaint",

    "lot": "Logistics",
    "product": "Logistics",
    "abbott": "Manufacturer",
    "dexcom": "Manufacturer",
    "tandem": "Manufacturer",
}

"""## Step 1B: Add Category Tags to Each FOI Row"""

import numpy as np

def tag_issue_categories(text, keyword_map):
    if pd.isna(text):
        return []
    text = text.lower()
    matched_categories = set()
    for keyword, category in keyword_map.items():
        if keyword.lower() in text:
            matched_categories.add(category)
    return list(matched_categories)

# Apply to your DataFrame
df_filtered['issue_categories'] = df_filtered['cleaned_foitext'].apply(lambda x: tag_issue_categories(x, keyword_to_category))

# Example output
print(df_filtered[['BRAND_NAME', 'issue_categories']].head())

def generate_summary(row):
    brand = row['BRAND_NAME']
    categories = ", ".join(row['issue_categories']) if row['issue_categories'] else "Uncategorized"
    snippet = row['cleaned_foitext'][:300] if pd.notna(row['cleaned_foitext']) else "No FOI text available"
    return f"Device: {brand} | Issues: {categories} | Description: {snippet}"

df_filtered['summary_chunk'] = df_filtered.apply(generate_summary, axis=1)

# Show sample
df_filtered[['summary_chunk']].sample(5, random_state=42)

# Export for manual check (optional)
df_filtered[['MDR_REPORT_KEY', 'BRAND_NAME', 'issue_categories', 'summary_chunk']].to_csv(
    '/content/drive/MyDrive/Extracted_FDA_Data/final_merged_data/device_foi_chunks_2024.csv',
    index=False
)

!pip install -q sentence-transformers

from sentence_transformers import SentenceTransformer
import numpy as np

# Load model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Encode summaries in batches (to save memory)
batch_size = 2048
embeddings = []

summary_chunks = df_filtered['summary_chunk'].tolist()

for i in range(0, len(summary_chunks), batch_size):
    batch = summary_chunks[i:i+batch_size]
    emb = model.encode(batch, show_progress_bar=True)
    embeddings.extend(emb)

# Convert to NumPy array
embeddings_np = np.array(embeddings)

# Save metadata for reference (adjust columns as needed)
df_filtered[['MDR_REPORT_KEY', 'BRAND_NAME', 'issue_categories', 'summary_chunk']].to_csv(
    "/content/drive/MyDrive/Extracted_FDA_Data/final_merged_data/embedded_foi_meta_2024.csv",
    index=False
)

# Save embeddings as compressed NumPy file
np.savez_compressed(
    "/content/drive/MyDrive/Extracted_FDA_Data/final_merged_data/embedded_foi_2024.npz",
    embeddings=embeddings_np
)

print("‚úÖ Both metadata and embeddings saved successfully.")

# Load saved metadata
df_meta = pd.read_csv("/content/drive/MyDrive/Extracted_FDA_Data/final_merged_data/embedded_foi_meta_2024.csv")

# Load saved embeddings
data = np.load("/content/drive/MyDrive/Extracted_FDA_Data/final_merged_data/embedded_foi_2024.npz")
embeddings_np = data['embeddings']

from sklearn.metrics.pairwise import cosine_similarity

def search_foi(query, model, embeddings, df_meta, top_k=5):
    # Encode the query
    query_embedding = model.encode([query])

    # Compute cosine similarity
    similarities = cosine_similarity(query_embedding, embeddings)[0]

    # Get top K results
    top_indices = similarities.argsort()[::-1][:top_k]

    # Return top matches with similarity scores
    results = df_meta.iloc[top_indices].copy()
    results['similarity'] = similarities[top_indices]
    return results

query = "Which device had issues with cannula dislodgement?"

results = search_foi(
    query=query,
    model=model,
    embeddings=embeddings_np,
    df_meta=df_meta,
    top_k=5  # You can increase to 10 later
)

pd.set_option('display.max_colwidth', None)  # Show full text
print("üîç Top Matching Results:\n")
display(results[['BRAND_NAME', 'issue_categories', 'summary_chunk', 'similarity']])

"""## NEXT STEP"""

import re
import nltk
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sentence_transformers import SentenceTransformer, util

nltk.download('punkt')
nltk.download('stopwords')

def extract_keywords(text, top_n=3):
    """Extracts top keywords from the user query using simple token frequency"""
    tokens = word_tokenize(text.lower())
    filtered = [w for w in tokens if w.isalpha() and w not in stopwords.words('english')]
    freq = pd.Series(filtered).value_counts()
    return freq.head(top_n).index.tolist()

def answer_query_insightfully(query, df, embeddings, top_k=10, plot_chart=True):
    # Load model
    model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')
    query_embedding = model.encode(query, convert_to_tensor=True)

    # Compute cosine similarity
    embeddings_tensor = torch.tensor(embeddings).to('cpu')
    similarities = util.cos_sim(query_embedding, embeddings_tensor)[0].numpy()

    # Score + sort
    df_temp = df.copy()
    df_temp["similarity"] = similarities
    df_sorted = df_temp.sort_values(by="similarity", ascending=False)

    # Extract keywords from query
    extracted_keywords = extract_keywords(query)
    keyword_pattern = '|'.join([re.escape(k) for k in extracted_keywords])
    df_sorted = df_sorted[df_sorted["summary_chunk"].str.contains(keyword_pattern, case=False, na=False)]

    if df_sorted.empty:
        return "‚ùå Sorry, no relevant results found for your query.", pd.DataFrame()

    # Top results & count by device
    top_results = df_sorted.head(top_k)
    device_counts = top_results["BRAND_NAME"].value_counts()
    top_device = device_counts.idxmax()
    top_device_count = device_counts.max()

    # Clean issue list
    issue_list_raw = df_sorted[df_sorted['BRAND_NAME'] == top_device]['issue_categories'].values[0]
    issue_list_cleaned = issue_list_raw.strip("[]").replace("'", "").split(", ") if isinstance(issue_list_raw, str) else ["Not available"]

    # Examples
    example_rows = top_results[top_results["BRAND_NAME"] == top_device].head(3)
    examples = example_rows["summary_chunk"].astype(str).apply(lambda x: x.split('.')[0].strip()).tolist()

    # Text-based bar chart
    chart = "\nüìä **Relevant Devices:**\n"
    for device, count in device_counts.items():
        bar = "‚ñà" * (count * 10 // top_device_count)
        chart += f"{device:<60} | {bar} ({count})\n"

    # Matplotlib chart
    if plot_chart:
        plt.figure(figsize=(10, 4))
        device_counts.sort_values(ascending=True).plot(kind='barh', color='skyblue')
        plt.title("Top Devices for Query")
        plt.xlabel("Relevant Report Count")
        plt.tight_layout()
        plt.show()

    # Final answer
    reply = f"""üîé Based on your question, the device **{top_device}** had the highest number of relevant reports.
It appeared in **~{top_device_count}** top results related to your query.

üßæ Common issues reported:
- {', '.join(issue_list_cleaned[:3])}

üìù **Example Reports:**\n""" + "\n".join([f"{i+1}. *\"{ex}...\"*" for i, ex in enumerate(examples)])

    return reply, top_results[["MDR_REPORT_KEY", "BRAND_NAME", "issue_categories", "summary_chunk", "similarity"]]

question = "Which sensor had the most battery failures?"
response, results_df = answer_query_insightfully(question, df_meta, embeddings_np, top_k=20)
print(response)

insight, top_df = answer_query_insightfully(
    "Which device had battery problems?",
    df_meta,
    embeddings_np,
    top_k=5
)

print(insight)

"""## NEXT STEP

# FINAL STEP
"""

!pip install streamlit sentence-transformers
!apt-get install -y libglib2.0-0 libsm6 libxrender1 libxext6

!mkdir -p /content/ollama_chatbot

!cp /content/drive/MyDrive/Extracted_FDA_Data/final_merged_data/embedded_foi_2024.npz /content/ollama_chatbot/embeddings.npz
!cp /content/drive/MyDrive/Extracted_FDA_Data/final_merged_data/embedded_foi_meta_2024.csv /content/ollama_chatbot/embedded_foi_meta.csv

!pip install -q streamlit pyngrok

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/ollama_chatbot/app.py
# import streamlit as st
# import pandas as pd
# import numpy as np
# import torch
# from sentence_transformers import SentenceTransformer, util
# import matplotlib.pyplot as plt
# 
# # Load data
# df = pd.read_csv("/content/drive/MyDrive/Extracted_FDA_Data/final_merged_data/embedded_foi_meta_2024.csv")
# embeddings = np.load("/content/drive/MyDrive/Extracted_FDA_Data/final_merged_data/embedded_foi_2024.npz")["embeddings"]
# 
# # Load model
# model = SentenceTransformer("all-MiniLM-L6-v2", device="cpu")
# 
# # Store conversation
# if "history" not in st.session_state:
#     st.session_state.history = []
# 
# def get_response(query, top_k=5, keyword_filter=None):
# 
#     query_embedding = model.encode(query, convert_to_tensor=True)
#     similarities = util.cos_sim(query_embedding, torch.tensor(embeddings))[0].numpy()
# 
#     df_temp = df.copy()
#     df_temp["similarity"] = similarities
#     df_sorted = df_temp.sort_values("similarity", ascending=False)
# 
#     positive_keywords = ["successful", "best", "liked", "safest", "effective"]
#     negative_indicators = ["malfunction", "complaint", "risk", "failure", "damaged", "issue"]
# 
#     if any(pos_kw in query.lower() for pos_kw in positive_keywords):
#         st.warning("‚ö†Ô∏è This dataset only includes FDA adverse event reports, so we can't determine true 'success' or satisfaction. Results reflect reported problems only.")
# 
#     # Then filter out obviously negative examples when the query is positive
#     if any(pos_kw in query.lower() for pos_kw in positive_keywords):
#         df_sorted = df_sorted[~df_sorted["summary_chunk"].str.contains("|".join(negative_indicators), case=False, na=False)]
# 
#     if keyword_filter:
#         df_sorted = df_sorted[df_sorted["summary_chunk"].str.contains(keyword_filter, case=False, na=False)]
# 
#     top_results = df_sorted.head(top_k)
#     device_counts = top_results["BRAND_NAME"].value_counts()
# 
#     if device_counts.empty:
#         return "‚ùå Sorry, I couldn‚Äôt find any relevant results for your query.", []
# 
#     top_device = device_counts.idxmax()
#     top_device_count = device_counts.max()
# 
#     example_rows = top_results[top_results["BRAND_NAME"] == top_device].head(3)
#     cleaned_examples = []
#     for text in example_rows["summary_chunk"]:
#         sentence = text.split(".")[0].strip()
#         cleaned = sentence.replace("Device:", "").replace("Description:", "").replace("|", "‚Ä¢").replace("Issues:", "Reported issues:")
#         cleaned_examples.append(cleaned)
# 
#     try:
#         issue_list_raw = top_results[top_results['BRAND_NAME'] == top_device]['issue_categories'].values[0]
#         issue_list_clean = issue_list_raw.strip("[]").replace("'", "").split(", ")
#     except:
#         issue_list_clean = ["Not available"]
# 
#     topic_term = query.strip("?").split()[-1]
#     reply = f"""
# üß† **Insight:**
# The **{top_device}** appears most relevant to your query regarding **{topic_term}**, with around **{top_device_count} matching entries**.
# 
# ‚ö†Ô∏è **Frequent issues**: {', '.join(issue_list_clean[:3])}
# 
# üí¨ **Sample complaints:**
# """ + "\n".join([f"- _{ex}..._" for ex in cleaned_examples])
# 
#     # Chart
#     with st.expander("üìä See Top Matching Devices"):
#         fig, ax = plt.subplots()
#         top_device_counts = device_counts.head(5)
#         ax.barh(top_device_counts.index[::-1], top_device_counts.values[::-1], color="skyblue")
#         ax.set_xlabel("Match Count")
#         ax.set_title("Top Devices by Relevance")
#         st.pyplot(fig)
# 
#     return reply
# 
# st.set_page_config(page_title="InsightPod", layout="wide")
# st.title("ü§ñ InsightPod ‚Äî Glucose Monitor Insight Assistant")
# 
# user_input = st.chat_input("Ask about glucose monitor issues...")
# 
# if user_input:
#     with st.spinner("üîç Scanning FOI reports..."):
#         response = get_response(user_input)
#         st.session_state.history.append(("You", user_input))
#         st.session_state.history.append(("InsightPod", response))
# 
# # Show chat history
# for speaker, message in st.session_state.history:
#     if speaker == "You":
#         st.markdown(f"**üßë‚Äçüí¨ You:** {message}")
#     else:
#         st.markdown(f"**ü§ñ InsightPod:** {message}")

!ngrok config add-authtoken 30hFCUi4A0FwFCeMDr86wvl1E9z_5TKJmuLUg21UHLVULpNPE

from pyngrok import ngrok
ngrok.kill()

!streamlit run /content/ollama_chatbot/app.py &>/content/logs.txt &

public_url = ngrok.connect(8501, "http")
print(f"üîó Public URL: {public_url}")

